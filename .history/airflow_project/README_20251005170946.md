# Projet ETL Qualité des Données Énergétiques (Apache Airflow)

## 🎯 Objectif du Projet
Ce projet démontre un pipeline ETL complet utilisant Apache Airflow pour **évaluer et améliorer la qualité des données** de consommation énergétique. Il intègre plusieurs sources hétérogènes (Paris, Évry, CSP, IRIS) dans des tables cibles unifiées, avec un focus sur la qualité des données.

## 📊 Schéma Cible
- **Consommation_IRIS_Paris** : `(ID_IRIS, Conso_moyenne_annuelle)`
- **Consommation_IRIS_Evry** : `(ID_IRIS, Conso_moyenne_annuelle)`
- **Consommation_CSP** : `(ID_CSP, Conso_moyenne_annuelle, Salaire_Moyen)`

## 📁 Sources de Données

### Source S1 – Données Paris
- **Population** : `(ID_Personne, Nom, Prénom, Adresse, CSP)`
- **Consommation** : `(ID_Adr, N, Nom_Rue, Code_Postal, NB_KW_Jour)`

### Source S2 – Données Évry
- **Population** : `(ID_Personne, Nom, Prénom, Adresse, CSP)`
- **Consommation** : `(ID_Adr, N, Nom_Rue, Code_Postal, NB_KW_Jour)`

### Source S3 – Catégories Socio-Professionnelles
- **CSP** : `(ID_CSP, Desc, Salaire_Moyen, Salaire_Min, Salaire_Max)`

### Source S4 – IRIS (Îlots Regroupés pour l'Information Statistique)
- **IRIS** : `(ID_Rue, ID_Ville, ID_IRIS)`

## 🔍 Évaluation de la Qualité des Données

Le système évalue automatiquement plusieurs dimensions de qualité :

### 1. **Conformité à un Format/Codification**
- Validation des codes CSP (1, 2, 3, 4)
- Vérification des formats numériques
- Consistance des codes postaux par ville

### 2. **Hétérogénéité des Échelles/Granularité**
- Analyse de la distribution des données par groupe
- Détection d'incohérences de granularité
- Normalisation des unités de mesure

### 3. **Complétude des Données**
- Calcul du pourcentage de valeurs manquantes par colonne
- Score de complétude global
- Stratégies de comblement des lacunes

### 4. **Détection et Élimination de Doublons**
- Identification des lignes dupliquées
- Calcul du score d'unicité
- Suppression intelligente des doublons

## 🏗️ Structure du DAG Airflow

```
📋 energy_data_quality_etl
├── 📤 Extraction des sources (4 sources)
├── 🔍 Évaluation qualité (métriques + visualisations)
├── 🔀 Décision stratégie nettoyage
├── 🧹 Amélioration qualité (si nécessaire)
├── 🔗 Intégration données
└── 📊 Rapport final
```

### Détail des Tâches

1. **Extraction** : Chargement des CSV sources avec statistiques
2. **Évaluation Qualité** : 
   - Génération de rapports JSON détaillés
   - Création de visualisations (dashboards)
   - Calcul de scores par dimension
3. **Stratégie Conditionnelle** : Branchement automatique selon la qualité
4. **Amélioration** : Application de règles de nettoyage spécifiques
5. **Intégration** : Jointures et agrégations vers les tables cibles
6. **Reporting** : Synthèse globale et métriques finales

## 📈 Métriques et Visualisations

Le système génère automatiquement :

- **Dashboards qualité** : Graphiques de complétude, distribution des erreurs
- **Rapports JSON** : Métriques détaillées par source et dimension
- **Scores globaux** : Note de qualité de 0 à 100 avec niveau (Excellent, Bon, Moyen, Faible)
- **Logs d'amélioration** : Traçabilité des corrections appliquées

## 🚀 Comment Exécuter le Projet

### Prérequis
- Docker et Docker Compose
- 4GB RAM minimum
- 2 CPU minimum

### Démarrage Rapide

1. **Cloner et naviguer dans le projet**
   ```bash
   cd airflow_project
   ```

2. **Initialiser Airflow avec Docker**
   ```bash
   docker-compose up airflow-init
   ```

3. **Démarrer les services**
   ```bash
   docker-compose up -d
   ```

4. **Accéder à l'interface Airflow**
   - URL : http://localhost:8080
   - Login : admin / admin123

5. **Déclencher le DAG**
   - Naviguer vers "energy_data_quality_etl"
   - Cliquer sur "Trigger DAG"

### Alternative : Installation Locale

```bash
pip install -r requirements.txt
export AIRFLOW_HOME=$(pwd)
airflow db init
airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
airflow webserver --port 8080 &
airflow scheduler &
```

## 📂 Structure du Projet

```
airflow_project/
├── dags/
│   └── energy_quality_etl.py          # DAG principal
├── sources/                           # Données sources (avec défauts)
│   ├── population_paris.csv
│   ├── population_evry.csv
│   ├── consommation_paris.csv
│   ├── consommation_evry.csv
│   ├── csp.csv
│   └── iris.csv
├── output/                           # Données nettoyées et tables cibles
│   ├── consommation_iris_paris.csv
│   ├── consommation_iris_evry.csv
│   └── consommation_csp.csv
├── quality_reports/                  # Rapports qualité JSON
├── visualizations/                   # Graphiques et dashboards
├── utils/
│   ├── quality_checks.py            # Outils d'évaluation qualité
│   └── data_quality_improver.py     # Outils d'amélioration
├── docker-compose.yml               # Configuration Docker
├── requirements.txt
└── README.md
```

## 🔧 Logique de Transformation

### Jointures et Agrégations

1. **Population + Consommation** : Jointure sur les adresses/IDs
2. **+ IRIS** : Enrichissement géographique par rue
3. **Agrégation par IRIS** : Calcul de consommation moyenne par zone
4. **Agrégation par CSP** : Analyse sociologique + salaires moyens

### Opérations Algébriques

- **Moyenne de consommation** : `SUM(NB_KW_Jour) / COUNT(*) * 365` (annualisation)
- **Score de qualité** : Moyenne pondérée des dimensions (Complétude 30%, Unicité 25%, Format 25%, Codification 20%)
- **Normalisation CSP** : Mapping textuel vers codes numériques

## 🎯 Améliorations de Qualité Implémentées

### Complétude
- Valeurs manquantes → Stratégies adaptées (moyenne, mode, valeurs par défaut)
- Traçabilité des imputations

### Conformité
- Codes CSP invalides → Normalisation automatique
- Formats incohérents → Standardisation (TitleCase, numeric)

### Unicité
- Doublons exacts → Suppression avec keep='first'
- Log du nombre de lignes supprimées

### Codification
- Mapping CSP texte/numérique : `{cadre: 1, employé: 2, ouvrier: 3, retraité: 4}`
- Validation des codes postaux par ville

## 📊 Interprétation des Résultats

- **Score > 90%** : Qualité Excellente ✅
- **Score 80-90%** : Qualité Bonne 🟢  
- **Score 70-80%** : Qualité Correcte 🟡
- **Score < 70%** : Qualité Insuffisante 🔴 (nettoyage automatique)

## 🔍 Monitoring et Logs

Tous les traitements sont loggés avec :
- Horodatage des opérations
- Nombre de lignes affectées
- Détail des corrections appliquées
- Métriques avant/après amélioration

## 🛠️ Extensions Possibles

- Intégration PostgreSQL pour persistance
- Alertes automatiques sur dégradation qualité
- API REST pour consultation des métriques
- Intégration avec outils BI (Tableau, PowerBI)
- Tests unitaires et validation de schema

---
*Projet développé pour démontrer les meilleures pratiques ETL avec focus qualité des données*
