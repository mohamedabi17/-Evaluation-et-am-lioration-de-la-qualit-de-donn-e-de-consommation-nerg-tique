#!/usr/bin/env python3
"""
Moniteur de performance pour les op√©rations ETL sur gros volumes
Ce script mesure les performances des op√©rations de qualit√© sur diff√©rentes tailles de donn√©es
"""

import time
import psutil
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
from datetime import datetime
import sys
import os

# Ajouter le r√©pertoire utils au path
sys.path.append('utils')

from quality_checks import generate_quality_report, create_quality_visualizations
from data_quality_improver import DataQualityImprover, create_improvement_config_population, create_improvement_config_consommation

class ETLPerformanceMonitor:
    """Moniteur de performance pour les op√©rations ETL."""
    
    def __init__(self):
        self.metrics = []
        self.start_time = None
        self.start_memory = None
        self.start_cpu = None
    
    def start_monitoring(self, operation_name):
        """D√©marrer le monitoring d'une op√©ration."""
        self.start_time = time.time()
        self.start_memory = psutil.virtual_memory().used / (1024**2)  # MB
        self.start_cpu = psutil.cpu_percent()
        
        return {
            'operation': operation_name,
            'start_time': self.start_time,
            'start_memory_mb': self.start_memory,
            'start_cpu_percent': self.start_cpu
        }
    
    def stop_monitoring(self, operation_data, rows_processed=0, additional_metrics=None):
        """Arr√™ter le monitoring et enregistrer les m√©triques."""
        end_time = time.time()
        end_memory = psutil.virtual_memory().used / (1024**2)
        end_cpu = psutil.cpu_percent()
        
        duration = end_time - operation_data['start_time']
        memory_usage = end_memory - operation_data['start_memory_mb']
        
        metrics = {
            'operation': operation_data['operation'],
            'duration_seconds': round(duration, 2),
            'memory_usage_mb': round(memory_usage, 2),
            'cpu_usage_percent': round((operation_data['start_cpu_percent'] + end_cpu) / 2, 2),
            'rows_processed': rows_processed,
            'rows_per_second': round(rows_processed / duration, 2) if duration > 0 else 0,
            'timestamp': datetime.now().isoformat()
        }
        
        if additional_metrics:
            metrics.update(additional_metrics)
        
        self.metrics.append(metrics)
        return metrics

def benchmark_data_loading():
    """Benchmark du chargement de donn√©es."""
    
    print("üìä BENCHMARK - Chargement de Donn√©es")
    print("-" * 40)
    
    monitor = ETLPerformanceMonitor()
    sources_dir = Path("sources")
    
    if not sources_dir.exists():
        print("‚ùå Dossier 'sources' non trouv√©")
        return {}
    
    results = {}
    
    for csv_file in sources_dir.glob("*.csv"):
        print(f"üìÇ Chargement de {csv_file.name}...")
        
        # Mesurer la taille du fichier
        file_size_mb = csv_file.stat().st_size / (1024**2)
        
        # D√©marrer le monitoring
        op_data = monitor.start_monitoring(f"load_{csv_file.stem}")
        
        try:
            # Charger les donn√©es
            df = pd.read_csv(csv_file)
            
            # Arr√™ter le monitoring
            metrics = monitor.stop_monitoring(
                op_data, 
                rows_processed=len(df),
                additional_metrics={
                    'file_size_mb': round(file_size_mb, 2),
                    'columns': len(df.columns),
                    'mb_per_second': round(file_size_mb / (time.time() - op_data['start_time']), 2)
                }
            )
            
            results[csv_file.stem] = metrics
            
            print(f"  ‚úÖ {len(df):,} lignes en {metrics['duration_seconds']}s "
                  f"({metrics['rows_per_second']:,.0f} lignes/s)")
            
        except Exception as e:
            print(f"  ‚ùå Erreur: {e}")
    
    return results

def benchmark_quality_evaluation():
    """Benchmark de l'√©valuation de qualit√©."""
    
    print("\nüîç BENCHMARK - √âvaluation de Qualit√©")
    print("-" * 40)
    
    monitor = ETLPerformanceMonitor()
    sources_dir = Path("sources")
    results = {}
    
    for csv_file in sources_dir.glob("*.csv"):
        if csv_file.name.startswith(('population', 'consommation')):
            print(f"üîç √âvaluation qualit√© de {csv_file.name}...")
            
            # Charger les donn√©es
            df = pd.read_csv(csv_file)
            
            # D√©marrer le monitoring
            op_data = monitor.start_monitoring(f"quality_{csv_file.stem}")
            
            try:
                # G√©n√©rer le rapport de qualit√©
                report = generate_quality_report(df, csv_file.stem)
                
                # Arr√™ter le monitoring
                metrics = monitor.stop_monitoring(
                    op_data,
                    rows_processed=len(df),
                    additional_metrics={
                        'quality_score': report['quality_score']['overall_quality_score'],
                        'completeness_score': report['quality_score']['completeness_score'],
                        'uniqueness_score': report['quality_score']['uniqueness_score']
                    }
                )
                
                results[csv_file.stem] = metrics
                
                print(f"  ‚úÖ Score qualit√©: {metrics['quality_score']:.1f}% "
                      f"en {metrics['duration_seconds']}s")
                
            except Exception as e:
                print(f"  ‚ùå Erreur: {e}")
    
    return results

def benchmark_data_improvement():
    """Benchmark de l'am√©lioration de qualit√©."""
    
    print("\nüßπ BENCHMARK - Am√©lioration de Qualit√©")
    print("-" * 40)
    
    monitor = ETLPerformanceMonitor()
    sources_dir = Path("sources")
    results = {}
    
    for csv_file in sources_dir.glob("population*.csv"):
        print(f"üßπ Am√©lioration de {csv_file.name}...")
        
        # Charger les donn√©es
        df = pd.read_csv(csv_file)
        original_quality = generate_quality_report(df, "temp")['quality_score']['overall_quality_score']
        
        # D√©marrer le monitoring
        op_data = monitor.start_monitoring(f"improve_{csv_file.stem}")
        
        try:
            # Appliquer les am√©liorations
            improver = DataQualityImprover()
            config = create_improvement_config_population()
            
            # S√©rie d'am√©liorations
            df_improved = improver.remove_duplicates(df)
            df_improved = improver.improve_completeness(df_improved, config['completeness_strategies'])
            df_improved = improver.standardize_format(df_improved, config['format_rules'])
            df_improved = improver.normalize_codification(df_improved, config['codification_rules'])
            df_improved = improver.apply_business_rules(df_improved, config['business_rules'])
            
            # Calculer la qualit√© finale
            final_quality = generate_quality_report(df_improved, "temp")['quality_score']['overall_quality_score']
            quality_gain = final_quality - original_quality
            
            # Arr√™ter le monitoring
            metrics = monitor.stop_monitoring(
                op_data,
                rows_processed=len(df),
                additional_metrics={
                    'original_quality': round(original_quality, 1),
                    'final_quality': round(final_quality, 1),
                    'quality_gain': round(quality_gain, 1),
                    'improvement_actions': len(improver.improvement_log)
                }
            )
            
            results[csv_file.stem] = metrics
            
            print(f"  ‚úÖ Qualit√©: {original_quality:.1f}% ‚Üí {final_quality:.1f}% "
                  f"(+{quality_gain:.1f}) en {metrics['duration_seconds']}s")
            
        except Exception as e:
            print(f"  ‚ùå Erreur: {e}")
    
    return results

def benchmark_aggregations():
    """Benchmark des op√©rations d'agr√©gation."""
    
    print("\nüìà BENCHMARK - Agr√©gations")
    print("-" * 40)
    
    monitor = ETLPerformanceMonitor()
    sources_dir = Path("sources")
    
    # Charger toutes les donn√©es n√©cessaires
    try:
        pop_paris = pd.read_csv(sources_dir / "population_paris.csv")
        pop_evry = pd.read_csv(sources_dir / "population_evry.csv")
        conso_paris = pd.read_csv(sources_dir / "consommation_paris.csv")
        conso_evry = pd.read_csv(sources_dir / "consommation_evry.csv")
        
        results = {}
        
        # 1. Agr√©gation par CSP
        print("üìä Agr√©gation par CSP...")
        op_data = monitor.start_monitoring("aggregation_csp")
        
        pop_all = pd.concat([pop_paris, pop_evry])
        conso_all = pd.concat([conso_paris, conso_evry])
        
        # Jointure simplifi√©e pour le benchmark
        merged = pd.merge(
            pop_all[['ID_Personne', 'CSP']], 
            conso_all[['ID_Adr', 'NB_KW_Jour']], 
            left_on='ID_Personne', 
            right_on='ID_Adr', 
            how='inner'
        )
        
        # Agr√©gation
        agg_csp = merged.groupby('CSP')['NB_KW_Jour'].agg(['mean', 'count', 'std']).reset_index()
        
        metrics = monitor.stop_monitoring(
            op_data,
            rows_processed=len(merged),
            additional_metrics={
                'input_rows': len(pop_all) + len(conso_all),
                'joined_rows': len(merged),
                'output_groups': len(agg_csp)
            }
        )
        
        results['aggregation_csp'] = metrics
        print(f"  ‚úÖ {len(merged):,} lignes ‚Üí {len(agg_csp)} groupes CSP "
              f"en {metrics['duration_seconds']}s")
        
        # 2. Agr√©gation g√©ographique (simulation)
        print("üó∫Ô∏è  Agr√©gation g√©ographique...")
        op_data = monitor.start_monitoring("aggregation_geo")
        
        # Simulation d'agr√©gation g√©ographique
        geo_agg = conso_all.groupby('Code_Postal')['NB_KW_Jour'].agg(['mean', 'count', 'min', 'max']).reset_index()
        
        metrics = monitor.stop_monitoring(
            op_data,
            rows_processed=len(conso_all),
            additional_metrics={
                'input_rows': len(conso_all),
                'output_groups': len(geo_agg)
            }
        )
        
        results['aggregation_geo'] = metrics
        print(f"  ‚úÖ {len(conso_all):,} lignes ‚Üí {len(geo_agg)} groupes g√©ographiques "
              f"en {metrics['duration_seconds']}s")
        
        return results
        
    except Exception as e:
        print(f"‚ùå Erreur lors des agr√©gations: {e}")
        return {}

def create_performance_dashboard(all_results):
    """Cr√©er un dashboard de performance."""
    
    print("\nüìä Cr√©ation du Dashboard de Performance...")
    
    # Pr√©parer les donn√©es pour visualisation
    operations = []
    durations = []
    rows_processed = []
    memory_usage = []
    
    for category, results in all_results.items():
        for operation, metrics in results.items():
            operations.append(f"{category}_{operation}")
            durations.append(metrics['duration_seconds'])
            rows_processed.append(metrics['rows_processed'])
            memory_usage.append(metrics['memory_usage_mb'])
    
    # Cr√©er les graphiques
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. Dur√©es par op√©ration
    ax1.bar(operations, durations, color='skyblue', alpha=0.7)
    ax1.set_title('Dur√©e par Op√©ration (secondes)')
    ax1.set_ylabel('Dur√©e (s)')
    ax1.tick_params(axis='x', rotation=45)
    
    # 2. D√©bit (lignes/seconde)
    throughput = [r/d if d > 0 else 0 for r, d in zip(rows_processed, durations)]
    ax2.bar(operations, throughput, color='lightgreen', alpha=0.7)
    ax2.set_title('D√©bit (lignes/seconde)')
    ax2.set_ylabel('Lignes/s')
    ax2.tick_params(axis='x', rotation=45)
    
    # 3. Utilisation m√©moire
    ax3.bar(operations, memory_usage, color='coral', alpha=0.7)
    ax3.set_title('Utilisation M√©moire (MB)')
    ax3.set_ylabel('M√©moire (MB)')
    ax3.tick_params(axis='x', rotation=45)
    
    # 4. Performance vs Taille
    ax4.scatter(rows_processed, durations, c=memory_usage, cmap='viridis', alpha=0.7, s=100)
    ax4.set_xlabel('Lignes trait√©es')
    ax4.set_ylabel('Dur√©e (s)')
    ax4.set_title('Performance vs Taille des Donn√©es')
    ax4.set_xscale('log')
    
    plt.tight_layout()
    
    # Sauvegarder
    viz_dir = Path("visualizations")
    viz_dir.mkdir(exist_ok=True)
    plt.savefig(viz_dir / "performance_dashboard.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"  ‚úÖ Dashboard sauvegard√©: {viz_dir}/performance_dashboard.png")

def save_performance_report(all_results):
    """Sauvegarder un rapport de performance d√©taill√©."""
    
    reports_dir = Path("quality_reports")
    reports_dir.mkdir(exist_ok=True)
    
    # Calculer des statistiques globales
    all_metrics = []
    for category_results in all_results.values():
        all_metrics.extend(category_results.values())
    
    if not all_metrics:
        return
    
    total_duration = sum(m['duration_seconds'] for m in all_metrics)
    total_rows = sum(m['rows_processed'] for m in all_metrics)
    avg_throughput = total_rows / total_duration if total_duration > 0 else 0
    max_memory = max(m['memory_usage_mb'] for m in all_metrics)
    
    performance_report = {
        'benchmark_timestamp': datetime.now().isoformat(),
        'system_info': {
            'cpu_count': psutil.cpu_count(),
            'memory_total_gb': round(psutil.virtual_memory().total / (1024**3), 2),
            'python_version': sys.version
        },
        'global_metrics': {
            'total_duration_seconds': round(total_duration, 2),
            'total_rows_processed': total_rows,
            'average_throughput_rows_per_second': round(avg_throughput, 2),
            'peak_memory_usage_mb': round(max_memory, 2)
        },
        'detailed_results': all_results
    }
    
    # Sauvegarder le rapport
    report_file = reports_dir / f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(performance_report, f, indent=2, ensure_ascii=False)
    
    print(f"\nüìã Rapport de performance sauvegard√©: {report_file}")
    
    # Afficher le r√©sum√©
    print("\nüìä R√âSUM√â DE PERFORMANCE")
    print("-" * 30)
    print(f"Dur√©e totale: {total_duration:.1f}s")
    print(f"Lignes trait√©es: {total_rows:,}")
    print(f"D√©bit moyen: {avg_throughput:,.0f} lignes/s")
    print(f"Pic m√©moire: {max_memory:.1f} MB")

def main():
    """Fonction principale de benchmark."""
    
    print("üöÄ BENCHMARK DE PERFORMANCE ETL")
    print("=" * 50)
    print("Ce benchmark va mesurer les performances des op√©rations ETL")
    print("sur les donn√©es pr√©sentes dans le dossier 'sources'")
    print()
    
    # V√©rifier la pr√©sence de donn√©es
    sources_dir = Path("sources")
    if not sources_dir.exists() or not any(sources_dir.glob("*.csv")):
        print("‚ùå Aucune donn√©e trouv√©e dans 'sources'!")
        print("G√©n√©rez d'abord des donn√©es avec generate_large_datasets.py")
        return
    
    # Afficher les donn√©es disponibles
    print("üìÇ Donn√©es disponibles:")
    total_size = 0
    for csv_file in sources_dir.glob("*.csv"):
        size_mb = csv_file.stat().st_size / (1024**2)
        total_size += size_mb
        print(f"  {csv_file.name:25} : {size_mb:6.1f} MB")
    print(f"  {'TOTAL':25} : {total_size:6.1f} MB")
    print()
    
    # Demander confirmation
    proceed = input("Lancer le benchmark? (o/N): ").lower().strip()
    if proceed not in ['o', 'oui', 'y', 'yes']:
        print("‚ùå Benchmark annul√©")
        return
    
    start_time = time.time()
    
    # Lancer les diff√©rents benchmarks
    all_results = {}
    
    try:
        # 1. Benchmark chargement
        all_results['loading'] = benchmark_data_loading()
        
        # 2. Benchmark √©valuation qualit√©
        all_results['quality_evaluation'] = benchmark_quality_evaluation()
        
        # 3. Benchmark am√©lioration
        all_results['quality_improvement'] = benchmark_data_improvement()
        
        # 4. Benchmark agr√©gations
        all_results['aggregations'] = benchmark_aggregations()
        
        # 5. Cr√©er les visualisations
        create_performance_dashboard(all_results)
        
        # 6. Sauvegarder le rapport
        save_performance_report(all_results)
        
        total_time = time.time() - start_time
        print(f"\n‚úÖ Benchmark termin√© en {total_time:.1f}s")
        
    except Exception as e:
        print(f"\n‚ùå Erreur durant le benchmark: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()