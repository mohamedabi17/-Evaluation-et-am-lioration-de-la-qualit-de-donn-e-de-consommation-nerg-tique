#!/usr/bin/env python3
"""
Script de d√©monstration des am√©liorations de qualit√© des donn√©es
Ce script peut √™tre ex√©cut√© ind√©pendamment pour tester les fonctionnalit√©s
"""

import sys
import os
import pandas as pd
import json

# Ajouter le r√©pertoire utils au path
sys.path.append(os.path.join(os.path.dirname(__file__), 'utils'))

from quality_checks import generate_quality_report, create_quality_visualizations
from data_quality_improver import DataQualityImprover, create_improvement_config_population

def demonstrate_quality_improvement():
    """D√©monstration compl√®te du processus d'am√©lioration qualit√©."""
    
    print("üöÄ D√©monstration des Am√©liorations de Qualit√© des Donn√©es")
    print("=" * 60)
    
    # 1. Charger les donn√©es sources avec d√©fauts
    sources_path = "sources"
    pop_paris_path = os.path.join(sources_path, "population_paris.csv")
    
    if not os.path.exists(pop_paris_path):
        print(f"‚ùå Fichier {pop_paris_path} non trouv√©")
        return
    
    df_original = pd.read_csv(pop_paris_path)
    print(f"üìä Donn√©es originales charg√©es: {len(df_original)} lignes")
    
    # 2. √âvaluation qualit√© initiale
    print("\nüîç √âVALUATION QUALIT√â INITIALE")
    print("-" * 40)
    
    initial_report = generate_quality_report(df_original, "population_paris_original")
    initial_score = initial_report['quality_score']['overall_quality_score']
    
    print(f"Score global initial: {initial_score}% ({initial_report['quality_score']['quality_level']})")
    print(f"Compl√©tude: {initial_report['quality_score']['completeness_score']}%")
    print(f"Unicit√©: {initial_report['quality_score']['uniqueness_score']}%")
    print(f"Codification: {initial_report['quality_score']['codification_score']}%")
    
    # Afficher les probl√®mes d√©tect√©s
    print("\nüìã Probl√®mes d√©tect√©s:")
    for col, completeness in initial_report['completeness'].items():
        if completeness['missing_count'] > 0:
            print(f"  - {col}: {completeness['missing_count']} valeurs manquantes")
    
    duplicates = initial_report['duplicates']['duplicate_count']
    if duplicates > 0:
        print(f"  - {duplicates} doublons d√©tect√©s")
    
    # 3. Application des am√©liorations
    print("\nüßπ APPLICATION DES AM√âLIORATIONS")
    print("-" * 40)
    
    improver = DataQualityImprover()
    config = create_improvement_config_population()
    
    # Appliquer les am√©liorations √©tape par √©tape
    df_improved = df_original.copy()
    
    # √âlimination des doublons
    df_improved = improver.remove_duplicates(df_improved)
    
    # Am√©lioration de la compl√©tude
    df_improved = improver.improve_completeness(df_improved, config['completeness_strategies'])
    
    # Standardisation des formats
    df_improved = improver.standardize_format(df_improved, config['format_rules'])
    
    # Normalisation de la codification
    df_improved = improver.normalize_codification(df_improved, config['codification_rules'])
    
    # Application des r√®gles m√©tier
    df_improved = improver.apply_business_rules(df_improved, config['business_rules'])
    
    # 4. √âvaluation qualit√© finale
    print("\nüìà √âVALUATION QUALIT√â FINALE")
    print("-" * 40)
    
    final_report = generate_quality_report(df_improved, "population_paris_improved")
    final_score = final_report['quality_score']['overall_quality_score']
    
    print(f"Score global final: {final_score}% ({final_report['quality_score']['quality_level']})")
    print(f"Compl√©tude: {final_report['quality_score']['completeness_score']}%")
    print(f"Unicit√©: {final_report['quality_score']['uniqueness_score']}%")
    print(f"Codification: {final_report['quality_score']['codification_score']}%")
    
    # 5. R√©sum√© des am√©liorations
    improvement_gain = final_score - initial_score
    print(f"\nüéØ GAIN QUALIT√â: +{improvement_gain:.1f} points")
    
    print("\nüìù R√©sum√© des am√©liorations appliqu√©es:")
    summary = improver.get_improvement_summary()
    for category, count in summary['by_category'].items():
        print(f"  - {category}: {count} action(s)")
    
    # 6. Sauvegarder les r√©sultats
    output_dir = "output"
    os.makedirs(output_dir, exist_ok=True)
    
    # Sauvegarder les donn√©es am√©lior√©es
    improved_file = os.path.join(output_dir, "population_paris_improved_demo.csv")
    df_improved.to_csv(improved_file, index=False)
    print(f"\nüíæ Donn√©es am√©lior√©es sauvegard√©es: {improved_file}")
    
    # Sauvegarder les rapports
    reports_dir = "quality_reports"
    os.makedirs(reports_dir, exist_ok=True)
    
    with open(os.path.join(reports_dir, "demo_initial_report.json"), 'w', encoding='utf-8') as f:
        json.dump(initial_report, f, indent=2, ensure_ascii=False)
    
    with open(os.path.join(reports_dir, "demo_final_report.json"), 'w', encoding='utf-8') as f:
        json.dump(final_report, f, indent=2, ensure_ascii=False)
    
    # Cr√©er les visualisations
    viz_dir = "visualizations"
    os.makedirs(viz_dir, exist_ok=True)
    
    try:
        create_quality_visualizations(initial_report, viz_dir)
        create_quality_visualizations(final_report, viz_dir)
        print(f"üìä Visualisations cr√©√©es dans {viz_dir}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur lors de la cr√©ation des visualisations: {e}")
    
    print("\n‚úÖ D√©monstration termin√©e avec succ√®s!")
    print(f"üìà Am√©lioration globale: {initial_score:.1f}% ‚Üí {final_score:.1f}% (+{improvement_gain:.1f})")

def analyze_all_sources():
    """Analyser toutes les sources de donn√©es."""
    
    print("\nüîç ANALYSE COMPL√àTE DE TOUTES LES SOURCES")
    print("=" * 50)
    
    sources = [
        "population_paris.csv",
        "population_evry.csv", 
        "consommation_paris.csv",
        "consommation_evry.csv",
        "csp.csv",
        "iris.csv"
    ]
    
    results = {}
    
    for source in sources:
        source_path = os.path.join("sources", source)
        if os.path.exists(source_path):
            df = pd.read_csv(source_path)
            source_name = source.replace('.csv', '')
            report = generate_quality_report(df, source_name)
            results[source_name] = report['quality_score']['overall_quality_score']
            
            print(f"{source_name:25} : {report['quality_score']['overall_quality_score']:5.1f}% ({report['quality_score']['quality_level']})")
    
    # Score moyen
    avg_score = sum(results.values()) / len(results)
    print(f"\n{'SCORE MOYEN':25} : {avg_score:5.1f}%")
    
    # Identifier les sources probl√©matiques
    problematic = [name for name, score in results.items() if score < 80]
    if problematic:
        print(f"\n‚ö†Ô∏è  Sources n√©cessitant une am√©lioration: {', '.join(problematic)}")
    else:
        print(f"\n‚úÖ Toutes les sources ont une qualit√© acceptable (>80%)")

if __name__ == "__main__":
    print("üéØ Script de D√©monstration - Qualit√© des Donn√©es √ânerg√©tiques")
    print("Ce script d√©montre les capacit√©s d'√©valuation et d'am√©lioration de la qualit√©")
    print()
    
    try:
        # D√©monstration principale
        demonstrate_quality_improvement()
        
        # Analyse de toutes les sources
        analyze_all_sources()
        
    except Exception as e:
        print(f"‚ùå Erreur durante la d√©monstration: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nüèÅ Fin de la d√©monstration")