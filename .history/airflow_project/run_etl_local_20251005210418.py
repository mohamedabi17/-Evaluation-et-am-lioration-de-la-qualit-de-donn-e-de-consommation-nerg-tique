#!/usr/bin/env python3
"""
ETL Local Runner - Ex√©cute le pipeline ETL sans Docker
Simule le comportement du DAG Airflow en local pour demonstration
"""

import os
import sys
import pandas as pd
import json
from datetime import datetime
from pathlib import Path
import time

# Ajouter le r√©pertoire utils au path
sys.path.append('utils')

from quality_checks import (
    generate_quality_report, 
    create_quality_visualizations,
    check_completeness,
    check_duplicates,
    check_codification_consistency
)

from data_quality_improver import (
    DataQualityImprover,
    create_improvement_config_population,
    create_improvement_config_consommation
)

# Configuration des chemins (local)
SOURCES_PATH = "sources"
OUTPUT_PATH = "output"
QUALITY_REPORTS_PATH = "quality_reports"
VISUALIZATIONS_PATH = "visualizations"

class LocalETLRunner:
    """Classe pour ex√©cuter l'ETL en local."""
    
    def __init__(self):
        self.create_directories()
        self.execution_log = []
        self.start_time = datetime.now()
    
    def create_directories(self):
        """Cr√©er les dossiers de sortie."""
        for path in [OUTPUT_PATH, QUALITY_REPORTS_PATH, VISUALIZATIONS_PATH]:
            Path(path).mkdir(exist_ok=True)
        print("üìÅ Dossiers de sortie cr√©√©s")
    
    def log_step(self, step_name, details=None):
        """Logger une √©tape de l'ETL."""
        timestamp = datetime.now()
        log_entry = {
            'step': step_name,
            'timestamp': timestamp.isoformat(),
            'details': details
        }
        self.execution_log.append(log_entry)
        print(f"‚úÖ {step_name} - {timestamp.strftime('%H:%M:%S')}")
        if details:
            print(f"   {details}")
    
    def extract_source_data(self, source_name):
        """Extraire les donn√©es sources."""
        file_path = f"{SOURCES_PATH}/{source_name}.csv"
        
        if not os.path.exists(file_path):
            print(f"‚ùå Fichier {file_path} non trouv√©")
            return None
        
        df = pd.read_csv(file_path)
        
        stats = {
            'rows': len(df),
            'columns': len(df.columns),
            'file_size_kb': round(os.path.getsize(file_path) / 1024, 2)
        }
        
        self.log_step(f"Extraction {source_name}", 
                     f"{stats['rows']:,} lignes, {stats['columns']} colonnes, {stats['file_size_kb']} KB")
        
        return df, stats
    
    def evaluate_data_quality(self, source_name, df):
        """√âvaluer la qualit√© des donn√©es."""
        print(f"\nüîç √âvaluation qualit√©: {source_name}")
        
        # G√©n√©rer le rapport de qualit√©
        report_path = f"{QUALITY_REPORTS_PATH}/{source_name}_quality_report.json"
        quality_report = generate_quality_report(df, source_name, report_path)
        
        # Cr√©er les visualisations
        try:
            create_quality_visualizations(quality_report, VISUALIZATIONS_PATH)
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur visualisation: {e}")
        
        # Log des m√©triques importantes
        quality_score = quality_report['quality_score']
        
        details = (f"Score: {quality_score['overall_quality_score']}% ({quality_score['quality_level']}) | "
                  f"Compl√©tude: {quality_score['completeness_score']}% | "
                  f"Unicit√©: {quality_score['uniqueness_score']}%")
        
        self.log_step(f"Qualit√© √©valu√©e {source_name}", details)
        
        return quality_report
    
    def decide_cleaning_strategy(self, quality_reports):
        """D√©cider de la strat√©gie de nettoyage."""
        print(f"\nü§î D√©cision strat√©gie de nettoyage...")
        
        needs_cleaning = False
        quality_threshold = 99  # Seuil d'acceptabilit√© √† 99%
        
        for source_name, report in quality_reports.items():
            score = report['quality_score']['overall_quality_score']
            if score < quality_threshold:
                needs_cleaning = True
                print(f"   {source_name}: Score {score:.2f}% < {quality_threshold}% - Nettoyage requis")
            else:
                print(f"   {source_name}: Score {score:.2f}% ‚â• {quality_threshold}% - Qualit√© excellente")
        
        if needs_cleaning:
            self.log_step("Strat√©gie", f"Qualit√© < {quality_threshold}% - Am√©lioration activ√©e")
            return 'improve'
        else:
            self.log_step("Strat√©gie", f"Qualit√© ‚â• {quality_threshold}% - Int√©gration directe")
            return 'integrate'
    
    def improve_data_quality(self, datasets):
        """Am√©liorer la qualit√© des donn√©es."""
        print(f"\nüßπ Am√©lioration de la qualit√© des donn√©es...")
        
        improved_datasets = {}
        
        for source_name, df in datasets.items():
            print(f"\n   üîß Traitement de {source_name}...")
            print(f"      üìä Donn√©es initiales: {len(df):,} lignes")
            
            improver = DataQualityImprover()
            
            # S√©lectionner la configuration appropri√©e
            if 'population' in source_name:
                config = create_improvement_config_population()
            else:
                config = create_improvement_config_consommation()
            
            # Appliquer les am√©liorations √©tape par √©tape
            print(f"      üßπ Suppression des doublons...")
            df_improved = improver.remove_duplicates(df)
            print(f"         ‚úÖ {len(df) - len(df_improved)} doublons supprim√©s")
            
            print(f"      üìù Am√©lioration de la compl√©tude...")
            df_improved = improver.improve_completeness(df_improved, config['completeness_strategies'])
            
            print(f"      üé® Standardisation des formats...")
            df_improved = improver.standardize_format(df_improved, config['format_rules'])
            
            # Appliquer la codification seulement si elle existe
            if 'codification_rules' in config:
                print(f"      üî¢ Normalisation de la codification...")
                df_improved = improver.normalize_codification(df_improved, config['codification_rules'])
            
            print(f"      üìã Application des r√®gles m√©tier...")
            df_improved = improver.apply_business_rules(df_improved, config['business_rules'])
            
            # Sauvegarder les donn√©es am√©lior√©es
            output_file = f"{OUTPUT_PATH}/{source_name}_clean.csv"
            df_improved.to_csv(output_file, index=False)
            
            improved_datasets[source_name] = df_improved
            
            # Afficher le r√©sum√© d√©taill√© des am√©liorations
            improvement_summary = improver.get_improvement_summary()
            total_actions = sum(improvement_summary['by_category'].values())
            
            print(f"      üìä R√©sultat: {len(df_improved):,} lignes finales")
            print(f"      ‚úÖ {total_actions} actions d'am√©lioration appliqu√©es:")
            
            for category, count in improvement_summary['by_category'].items():
                print(f"         ‚Ä¢ {category}: {count} action(s)")
            
            self.log_step(f"Am√©lioration {source_name}", 
                         f"{total_actions} actions appliqu√©es - {len(df_improved):,} lignes finales")
        
        return improved_datasets
    
    def integrate_data(self, datasets):
        """Int√©grer les donn√©es pour cr√©er les tables cibles."""
        print(f"\nüîó Int√©gration des donn√©es...")
        
        # Charger les donn√©es de r√©f√©rence
        csp = pd.read_csv(f"{SOURCES_PATH}/csp.csv")
        iris = pd.read_csv(f"{SOURCES_PATH}/iris.csv")
        
        results = {}
        
        # 1. Cr√©er Consommation_IRIS_Paris
        if 'population_paris' in datasets and 'consommation_paris' in datasets:
            result_paris = self.create_consommation_iris_table(
                datasets['population_paris'], 
                datasets['consommation_paris'], 
                iris, 
                "paris"
            )
            results['consommation_iris_paris'] = result_paris
        
        # 2. Cr√©er Consommation_IRIS_Evry
        if 'population_evry' in datasets and 'consommation_evry' in datasets:
            result_evry = self.create_consommation_iris_table(
                datasets['population_evry'], 
                datasets['consommation_evry'], 
                iris, 
                "evry"
            )
            results['consommation_iris_evry'] = result_evry
        
        # 3. Cr√©er Consommation_CSP
        if all(k in datasets for k in ['population_paris', 'population_evry', 'consommation_paris', 'consommation_evry']):
            result_csp = self.create_consommation_csp_table(
                datasets['population_paris'], 
                datasets['population_evry'],
                datasets['consommation_paris'], 
                datasets['consommation_evry'], 
                csp
            )
            results['consommation_csp'] = result_csp
        
        self.log_step("Int√©gration", f"{len(results)} tables cibles cr√©√©es")
        return results
    
    def create_consommation_iris_table(self, population, consommation, iris, city):
        """Cr√©er la table Consommation_IRIS pour une ville."""
        
        # Jointure Population + Consommation (simplifi√©e sur ID)
        merged = pd.merge(
            population[['ID_Personne', 'Adresse']], 
            consommation[['ID_Adr', 'Nom_Rue', 'NB_KW_Jour']], 
            left_on='ID_Personne', 
            right_on='ID_Adr', 
            how='inner'
        )
        
        # Jointure avec IRIS
        merged = pd.merge(
            merged,
            iris[iris['ID_Ville'].str.lower() == city.lower()],
            left_on='ID_Adr',
            right_on='ID_Rue',
            how='inner'
        )
        
        # Agr√©gation par IRIS
        result = merged.groupby('ID_IRIS')['NB_KW_Jour'].agg(['mean', 'count']).reset_index()
        result.columns = ['ID_IRIS', 'Conso_moyenne_annuelle', 'Nb_logements']
        
        # Conversion en consommation annuelle
        result['Conso_moyenne_annuelle'] = (result['Conso_moyenne_annuelle'] * 365).round(2)
        
        # Sauvegarder
        output_file = f"{OUTPUT_PATH}/consommation_iris_{city}.csv"
        result.to_csv(output_file, index=False)
        
        self.log_step(f"Table IRIS {city.title()}", f"{len(result)} zones IRIS")
        
        return result
    
    def create_consommation_csp_table(self, pop_paris, pop_evry, conso_paris, conso_evry, csp):
        """Cr√©er la table Consommation_CSP globale."""
        
        # Combiner les donn√©es
        pop_all = pd.concat([pop_paris, pop_evry])
        conso_all = pd.concat([conso_paris, conso_evry])
        
        # Standardiser le type de la colonne CSP (convertir en string)
        pop_all['CSP'] = pop_all['CSP'].astype(str)
        csp['ID_CSP'] = csp['ID_CSP'].astype(str)
        
        # Jointure Population + Consommation
        merged = pd.merge(
            pop_all[['ID_Personne', 'CSP']], 
            conso_all[['ID_Adr', 'NB_KW_Jour']], 
            left_on='ID_Personne', 
            right_on='ID_Adr', 
            how='inner'
        )
        
        # Agr√©gation par CSP
        csp_consumption = merged.groupby('CSP')['NB_KW_Jour'].agg(['mean', 'count']).reset_index()
        csp_consumption.columns = ['CSP', 'Conso_moyenne_annuelle', 'Nb_personnes']
        
        # S'assurer que les types sont compatibles pour la jointure
        csp_consumption['CSP'] = csp_consumption['CSP'].astype(str)
        
        # Jointure avec la table CSP
        result = pd.merge(
            csp_consumption,
            csp[['ID_CSP', 'Desc', 'Salaire_Moyen']],
            left_on='CSP',
            right_on='ID_CSP',
            how='left'
        )
        
        # Conversion en consommation annuelle
        result['Conso_moyenne_annuelle'] = (result['Conso_moyenne_annuelle'] * 365).round(2)
        
        # S√©lection des colonnes finales
        result = result[['ID_CSP', 'Desc', 'Conso_moyenne_annuelle', 'Salaire_Moyen', 'Nb_personnes']]
        
        # Sauvegarder
        output_file = f"{OUTPUT_PATH}/consommation_csp.csv"
        result.to_csv(output_file, index=False)
        
        self.log_step("Table CSP", f"{len(result)} cat√©gories socio-professionnelles")
        
        return result
    
    def generate_final_report(self, target_tables):
        """G√©n√©rer un rapport final de l'ETL."""
        
        print(f"\nüìä G√©n√©ration du rapport final...")
        
        end_time = datetime.now()
        duration = end_time - self.start_time
        
        final_report = {
            'etl_execution': {
                'start_time': self.start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'duration_seconds': duration.total_seconds(),
                'status': 'completed'
            },
            'target_tables': {},
            'execution_log': self.execution_log
        }
        
        # Statistiques des tables cibles
        for table_name, df in target_tables.items():
            if df is not None and len(df) > 0:
                final_report['target_tables'][table_name] = {
                    'rows': len(df),
                    'columns': len(df.columns),
                    'file': f"{OUTPUT_PATH}/{table_name}.csv"
                }
        
        # Sauvegarder le rapport
        report_file = f"{QUALITY_REPORTS_PATH}/final_etl_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)
        
        self.log_step("Rapport final", f"ETL termin√© en {duration.total_seconds():.1f}s")
        
        return final_report

def run_local_etl():
    """Fonction principale pour ex√©cuter l'ETL en local."""
    
    print("üöÄ LANCEMENT DE L'ETL LOCAL")
    print("=" * 50)
    print("Simulation du DAG Airflow 'energy_data_quality_etl' en local")
    print("Traitement des donn√©es sans conteneurs Docker")
    print()
    
    # V√©rifier la pr√©sence des donn√©es sources
    sources_dir = Path(SOURCES_PATH)
    if not sources_dir.exists():
        print(f"‚ùå Dossier '{SOURCES_PATH}' non trouv√©!")
        return
    
    required_files = ['population_paris.csv', 'population_evry.csv', 
                     'consommation_paris.csv', 'consommation_evry.csv',
                     'csp.csv', 'iris.csv']
    
    missing_files = [f for f in required_files if not (sources_dir / f).exists()]
    if missing_files:
        print(f"‚ùå Fichiers manquants: {', '.join(missing_files)}")
        print("G√©n√©rez d'abord les donn√©es avec generate_large_datasets.py")
        return
    
    # Initialiser l'ETL
    etl = LocalETLRunner()
    
    try:
        # √âTAPE 1: Extraction
        print("\nüì• √âTAPE 1: EXTRACTION DES DONN√âES")
        print("-" * 40)
        
        datasets = {}
        extraction_stats = {}
        
        sources = ['population_paris', 'population_evry', 'consommation_paris', 'consommation_evry']
        
        for source in sources:
            result = etl.extract_source_data(source)
            if result:
                datasets[source] = result[0]  # DataFrame
                extraction_stats[source] = result[1]  # Stats
        
        # √âTAPE 2: √âvaluation de la qualit√©
        print("\nüîç √âTAPE 2: √âVALUATION DE LA QUALIT√â")
        print("-" * 40)
        
        quality_reports = {}
        for source_name, df in datasets.items():
            quality_reports[source_name] = etl.evaluate_data_quality(source_name, df)
        
        # √âTAPE 3: D√©cision de strat√©gie
        print("\nü§î √âTAPE 3: STRAT√âGIE DE TRAITEMENT")
        print("-" * 40)
        
        strategy = etl.decide_cleaning_strategy(quality_reports)
        
        # √âTAPE 4: Am√©lioration (conditionnelle)
        if strategy == 'improve':
            print("\nüßπ √âTAPE 4: AM√âLIORATION DE LA QUALIT√â")
            print("-" * 40)
            datasets = etl.improve_data_quality(datasets)
            
            # R√©√©valuation apr√®s am√©lioration
            print("\nüîç R√â√âVALUATION POST-AM√âLIORATION")
            print("-" * 35)
            for source_name, df in datasets.items():
                post_report = etl.evaluate_data_quality(f"{source_name}_improved", df)
                improvement = (post_report['quality_score']['overall_quality_score'] - 
                             quality_reports[source_name]['quality_score']['overall_quality_score'])
                print(f"   {source_name}: +{improvement:.1f} points de qualit√©")
        
        # √âTAPE 5: Int√©gration
        print("\nüîó √âTAPE 5: INT√âGRATION DES DONN√âES")
        print("-" * 40)
        
        target_tables = etl.integrate_data(datasets)
        
        # √âTAPE 6: Rapport final
        print("\nüìä √âTAPE 6: RAPPORT FINAL")
        print("-" * 40)
        
        final_report = etl.generate_final_report(target_tables)
        
        # Affichage du r√©sum√©
        print("\n‚úÖ ETL TERMIN√â AVEC SUCC√àS!")
        print("=" * 50)
        print(f"‚è±Ô∏è  Dur√©e totale: {final_report['etl_execution']['duration_seconds']:.1f} secondes")
        print(f"üìÇ Tables cr√©√©es: {len(final_report['target_tables'])}")
        
        for table_name, info in final_report['target_tables'].items():
            print(f"   üìä {table_name}: {info['rows']:,} lignes")
        
        print(f"\nüìÅ Fichiers de sortie dans:")
        print(f"   üìÇ {OUTPUT_PATH}/          (donn√©es finales)")
        print(f"   üìä {QUALITY_REPORTS_PATH}/ (rapports qualit√©)")
        print(f"   üìà {VISUALIZATIONS_PATH}/  (graphiques)")
        
        # Ouvrir les fichiers de sortie
        print(f"\nüîç APER√áU DES R√âSULTATS:")
        for table_name in target_tables.keys():
            file_path = f"{OUTPUT_PATH}/{table_name}.csv"
            if os.path.exists(file_path):
                df_preview = pd.read_csv(file_path)
                print(f"\nüìä {table_name.upper()}:")
                print(df_preview.head(3).to_string(index=False))
        
    except Exception as e:
        print(f"\n‚ùå ERREUR DURANT L'ETL: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    run_local_etl()