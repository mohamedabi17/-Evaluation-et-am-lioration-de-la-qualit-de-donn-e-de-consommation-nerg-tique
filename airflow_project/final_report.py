#!/usr/bin/env python3
"""
ğŸ‰ RAPPORT FINAL - RÃ©sumÃ© complet du projet ETL QualitÃ© des DonnÃ©es
Analyse des rÃ©sultats et recommandations d'amÃ©lioration
"""

import os
import json
import pandas as pd
from pathlib import Path
from datetime import datetime

def analyze_final_results():
    """Analyser tous les rÃ©sultats du projet."""
    
    print("ğŸ‰ RAPPORT FINAL - PROJET ETL QUALITÃ‰ DES DONNÃ‰ES")
    print("=" * 65)
    print(f"ğŸ“… Date d'exÃ©cution: {datetime.now().strftime('%d/%m/%Y %H:%M')}")
    print()
    
    # 1. Analyse des donnÃ©es sources
    print("ğŸ“Š ANALYSE DES DONNÃ‰ES SOURCES")
    print("-" * 40)
    
    sources_dir = Path("sources")
    if sources_dir.exists():
        csv_files = list(sources_dir.glob("*.csv"))
        total_size = sum(f.stat().st_size for f in csv_files) / (1024**2)  # MB
        total_rows = 0
        
        print(f"   ğŸ“ Fichiers sources: {len(csv_files)}")
        
        for csv_file in sorted(csv_files):
            try:
                df = pd.read_csv(csv_file)
                rows = len(df)
                total_rows += rows
                size_mb = csv_file.stat().st_size / (1024**2)
                print(f"   ğŸ“„ {csv_file.name:25} : {rows:7,} lignes ({size_mb:.1f} MB)")
            except:
                print(f"   âŒ {csv_file.name:25} : erreur lecture")
        
        print(f"   ğŸ“Š TOTAL: {total_rows:,} lignes ({total_size:.1f} MB)")
    
    # 2. Analyse des rÃ©sultats de qualitÃ©
    print(f"\nğŸ” ANALYSE DES Ã‰VALUATIONS DE QUALITÃ‰")
    print("-" * 45)
    
    quality_dir = Path("quality_reports")
    if quality_dir.exists():
        # Rapports "before" et "after"
        before_reports = list(quality_dir.glob("*_before_quality.json"))
        after_reports = list(quality_dir.glob("*_after_quality.json"))
        
        print(f"   ğŸ“‹ Rapports avant amÃ©lioration: {len(before_reports)}")
        print(f"   ğŸ“‹ Rapports aprÃ¨s amÃ©lioration: {len(after_reports)}")
        
        if before_reports and after_reports:
            print(f"\n   ğŸ“Š COMPARAISON AVANT/APRÃˆS:")
            
            improvements = []
            
            for before_file in sorted(before_reports):
                source_name = before_file.name.replace('_before_quality.json', '')
                after_file = quality_dir / f"{source_name}_after_quality.json"
                
                if after_file.exists():
                    try:
                        with open(before_file, 'r') as f:
                            report_before = json.load(f)
                        with open(after_file, 'r') as f:
                            report_after = json.load(f)
                        
                        score_before = report_before['quality_score']['overall_quality_score']
                        score_after = report_after['quality_score']['overall_quality_score']
                        improvement = score_after - score_before
                        
                        status = "ğŸ“ˆ" if improvement > 0 else "ğŸ“‰" if improvement < 0 else "â¡ï¸"
                        
                        print(f"      {status} {source_name:20}: {score_before:5.1f}% â†’ {score_after:5.1f}% ({improvement:+5.1f})")
                        
                        improvements.append({
                            'source': source_name,
                            'before': score_before,
                            'after': score_after,
                            'improvement': improvement
                        })
                        
                    except Exception as e:
                        print(f"      âŒ {source_name:20}: erreur lecture ({e})")
            
            if improvements:
                avg_improvement = sum(i['improvement'] for i in improvements) / len(improvements)
                positive_improvements = sum(1 for i in improvements if i['improvement'] > 0)
                
                print(f"\n   ğŸ“ˆ AmÃ©lioration moyenne: {avg_improvement:+.2f} points")
                print(f"   âœ… Sources amÃ©liorÃ©es: {positive_improvements}/{len(improvements)}")
    
    # 3. Analyse des fichiers de sortie
    print(f"\nğŸ“ ANALYSE DES FICHIERS DE SORTIE")
    print("-" * 40)
    
    output_dir = Path("output")
    if output_dir.exists():
        output_files = list(output_dir.glob("*.csv"))
        
        if output_files:
            total_output_size = 0
            total_output_rows = 0
            
            print(f"   ğŸ“„ Fichiers crÃ©Ã©s: {len(output_files)}")
            
            for output_file in sorted(output_files):
                try:
                    df = pd.read_csv(output_file)
                    rows = len(df)
                    total_output_rows += rows
                    size_kb = output_file.stat().st_size / 1024
                    total_output_size += size_kb
                    
                    print(f"   ğŸ“Š {output_file.name:30} : {rows:7,} lignes ({size_kb:6.1f} KB)")
                except:
                    print(f"   âŒ {output_file.name:30} : erreur lecture")
            
            print(f"   ğŸ’¾ TOTAL OUTPUT: {total_output_rows:,} lignes ({total_output_size:.1f} KB)")
        else:
            print("   âŒ Aucun fichier de sortie trouvÃ©")
    
    # 4. Analyse des visualisations
    print(f"\nğŸ“ˆ ANALYSE DES VISUALISATIONS")
    print("-" * 35)
    
    viz_dir = Path("visualizations")
    if viz_dir.exists():
        png_files = list(viz_dir.glob("*.png"))
        html_files = list(viz_dir.glob("*.html"))
        
        if png_files:
            total_viz_size = sum(f.stat().st_size for f in png_files) / 1024  # KB
            print(f"   ğŸ–¼ï¸  Graphiques PNG: {len(png_files)} ({total_viz_size:.1f} KB)")
        
        if html_files:
            print(f"   ğŸŒ Graphiques HTML: {len(html_files)}")
        
        if not png_files and not html_files:
            print("   âŒ Aucune visualisation trouvÃ©e")
    
    # 5. Ã‰valuation par rapport au seuil de 99%
    print(f"\nğŸ¯ Ã‰VALUATION SEUIL D'ACCEPTABILITÃ‰ (99%)")
    print("-" * 50)
    
    quality_threshold = 99.0
    conforming_sources = 0
    total_sources = 0
    
    if quality_dir.exists():
        after_reports = list(quality_dir.glob("*_after_quality.json"))
        total_sources = len(after_reports)
        
        for report_file in after_reports:
            try:
                with open(report_file, 'r') as f:
                    report = json.load(f)
                
                source_name = report_file.name.replace('_after_quality.json', '')
                score = report['quality_score']['overall_quality_score']
                
                if score >= quality_threshold:
                    conforming_sources += 1
                    status = "âœ… CONFORME"
                else:
                    gap = quality_threshold - score
                    status = f"âŒ Ã‰CART: -{gap:.1f}pts"
                
                print(f"   {status:20} {source_name:20}: {score:5.1f}%")
                
            except:
                print(f"   âŒ ERREUR          {source_name:20}: lecture impossible")
    
    if total_sources > 0:
        conformity_rate = (conforming_sources / total_sources) * 100
        print(f"\n   ğŸ“Š Taux de conformitÃ©: {conforming_sources}/{total_sources} ({conformity_rate:.1f}%)")
        
        if conforming_sources == total_sources:
            print("   ğŸ‰ EXCELLENT: Toutes les sources respectent le seuil!")
        elif conforming_sources > total_sources // 2:
            print("   âš ï¸  ACCEPTABLE: MajoritÃ© des sources conformes")
        else:
            print("   ğŸ”§ INSUFFISANT: AmÃ©liorations majeures requises")
    
    # 6. Recommandations
    print(f"\nğŸ’¡ RECOMMANDATIONS D'AMÃ‰LIORATION")
    print("-" * 40)
    
    if conforming_sources < total_sources:
        print("   ğŸ”§ OPTIMISATIONS TECHNIQUES:")
        print("      â€¢ Revoir les rÃ¨gles de comblement des valeurs manquantes")
        print("      â€¢ Optimiser les stratÃ©gies de standardisation des formats")
        print("      â€¢ Ajuster les rÃ¨gles mÃ©tier pour Ã©viter la dÃ©gradation")
        print("      â€¢ ImplÃ©menter des validations plus strictes en amont")
        print()
    
    print("   ğŸ“ˆ AMÃ‰LIORATIONS FONCTIONNELLES:")
    print("      â€¢ DÃ©ployer Airflow avec Docker pour la production")
    print("      â€¢ IntÃ©grer des alertes de qualitÃ© en temps rÃ©el")
    print("      â€¢ Automatiser la gÃ©nÃ©ration de rapports exÃ©cutifs")
    print("      â€¢ ImplÃ©menter un dashboard de monitoring")
    print()
    
    print("   ğŸ¯ OBJECTIFS Ã€ LONG TERME:")
    print("      â€¢ Atteindre 99% de qualitÃ© sur toutes les sources")
    print("      â€¢ Automatiser la dÃ©tection d'anomalies")
    print("      â€¢ IntÃ©grer d'autres sources de donnÃ©es Ã©nergÃ©tiques")
    print("      â€¢ DÃ©velopper des modÃ¨les prÃ©dictifs de qualitÃ©")
    
    # 7. Bilan final
    print(f"\nğŸ† BILAN FINAL DU PROJET")
    print("=" * 35)
    
    print("   âœ… RÃ‰USSITES:")
    print("      â€¢ Pipeline ETL complet et fonctionnel")
    print("      â€¢ Ã‰valuation automatique de qualitÃ© des donnÃ©es")
    print("      â€¢ AmÃ©lioration intelligente des dÃ©fauts dÃ©tectÃ©s")
    print("      â€¢ Visualisations automatiques des mÃ©triques")
    print("      â€¢ Documentation complÃ¨te et exÃ©cutable")
    print("      â€¢ Alternative locale fonctionnelle (sans Docker)")
    
    if conforming_sources < total_sources:
        print("\n   ğŸ”§ DÃ‰FIS IDENTIFIÃ‰S:")
        print("      â€¢ Seuil de 99% trÃ¨s exigeant pour donnÃ©es rÃ©elles")
        print("      â€¢ Certaines amÃ©liorations peuvent dÃ©grader la qualitÃ©")
        print("      â€¢ Optimisation des rÃ¨gles mÃ©tier nÃ©cessaire")
        print("      â€¢ ConnectivitÃ© Docker Ã  rÃ©soudre pour dÃ©ploiement")
    
    print(f"\n   ğŸ¯ CONCLUSION:")
    if conforming_sources >= total_sources * 0.75:
        print("      ğŸ‰ PROJET RÃ‰USSI - Objectifs largement atteints")
    elif conforming_sources >= total_sources * 0.5:
        print("      âœ… PROJET SATISFAISANT - Objectifs partiellement atteints")
    else:
        print("      ğŸ”§ PROJET Ã€ OPTIMISER - AmÃ©liorations nÃ©cessaires")
    
    print(f"\n      ğŸ“Š Score projet: {(conforming_sources/total_sources*100) if total_sources > 0 else 0:.1f}%")
    print("      ğŸ’¡ Base solide pour amÃ©lirations futures")

def main():
    """GÃ©nÃ©rer le rapport final complet."""
    
    # VÃ©rifier qu'on est dans le bon rÃ©pertoire
    if not Path("sources").exists():
        print("âŒ Erreur: ExÃ©cutez ce script depuis le rÃ©pertoire du projet Airflow")
        return
    
    analyze_final_results()
    
    print(f"\nğŸ“‹ FICHIERS Ã€ CONSULTER:")
    print("   ğŸ“Š quality_reports/     - Rapports JSON dÃ©taillÃ©s")
    print("   ğŸ“ˆ visualizations/      - Graphiques de qualitÃ©")
    print("   ğŸ“ output/              - DonnÃ©es amÃ©liorÃ©es")
    print("   ğŸ“„ README.md            - Documentation mise Ã  jour")
    
    print(f"\nğŸ‰ Rapport final gÃ©nÃ©rÃ© avec succÃ¨s!")

if __name__ == "__main__":
    main()