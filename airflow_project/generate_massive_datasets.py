#!/usr/bin/env python3
"""
G√©n√©rateur de TR√àS GRANDES quantit√©s de donn√©es (Scale : Millions de lignes)
Ce script peut g√©n√©rer des datasets de taille industrielle pour tester les performances ETL
"""

import pandas as pd
import numpy as np
import random
from datetime import datetime, timedelta
import os
import sys
from pathlib import Path

# Configuration pour TR√àS GROS datasets
SCALE_CONFIGS = {
    'small': {
        'population_paris': 50_000,
        'population_evry': 25_000,
        'consumption_paris': 75_000,
        'consumption_evry': 35_000,
        'description': 'Configuration standard (~190k lignes, ~8MB)'
    },
    'medium': {
        'population_paris': 250_000,
        'population_evry': 150_000,
        'consumption_paris': 500_000,
        'consumption_evry': 300_000,
        'description': 'Configuration moyenne (~1.2M lignes, ~50MB)'
    },
    'large': {
        'population_paris': 1_000_000,
        'population_evry': 500_000,
        'consumption_paris': 2_000_000,
        'consumption_evry': 1_000_000,
        'description': 'Configuration importante (~4.5M lignes, ~200MB)'
    },
    'xlarge': {
        'population_paris': 2_500_000,
        'population_evry': 1_500_000,
        'consumption_paris': 5_000_000,
        'consumption_evry': 3_000_000,
        'description': 'Configuration tr√®s importante (~12M lignes, ~500MB)'
    },
    'xxlarge': {
        'population_paris': 5_000_000,
        'population_evry': 3_000_000,
        'consumption_paris': 10_000_000,
        'consumption_evry': 6_000_000,
        'description': 'Configuration industrielle (~24M lignes, ~1GB)'
    }
}

def generate_massive_population(city, size, chunk_size=50000):
    """G√©n√©rer une population massive par chunks pour √©viter les probl√®mes de m√©moire."""
    
    print(f"üë• G√©n√©ration MASSIVE de {size:,} habitants pour {city} (par chunks de {chunk_size:,})...")
    
    # G√©n√©rateur de noms/pr√©noms avec recyclage pour optimiser la m√©moire
    noms_cycle = random.choices([
        "Martin", "Bernard", "Dubois", "Thomas", "Robert", "Richard", "Petit", "Durand",
        "Leroy", "Moreau", "Simon", "Laurent", "Lefebvre", "Michel", "Garcia", "David",
        "Bertrand", "Roux", "Vincent", "Fournier", "Nguyen", "Lopez", "Gonzalez", "Rodriguez",
        "Chen", "Wang", "Li", "Zhang", "Kim", "Park", "Johnson", "Williams", "Brown", "Jones"
    ], k=1000)
    
    prenoms_cycle = random.choices([
        "Jean", "Marie", "Michel", "Alain", "Patrick", "Pierre", "Philippe", "Christophe",
        "Sophie", "Julie", "Camille", "Emma", "Lea", "Manon", "Lucas", "Hugo", "Louis",
        "Gabriel", "Arthur", "Raphael", "Mohamed", "Ahmed", "Hassan", "Fatima", "Aicha",
        "Sarah", "Leila", "David", "Daniel", "Michael", "Kevin", "Anthony", "Lisa"
    ], k=1000)
    
    # Adresses pr√©-g√©n√©r√©es pour √©viter la r√©p√©tition
    if city.lower() == "paris":
        rues_base = [
            "Rue de Rivoli", "Avenue des Champs-Elys√©es", "Boulevard Saint-Germain",
            "Rue de la Paix", "Avenue Montaigne", "Rue du Faubourg Saint-Honor√©",
            "Boulevard Haussmann", "Rue Lafayette", "Avenue Victor Hugo", "Rue Saint-Antoine"
        ]
        id_offset = 0
    else:
        rues_base = [
            "Rue de l'Essonne", "Avenue de la Gare", "Boulevard des Champs",
            "Rue de la R√©publique", "Avenue du Parc", "Boulevard Libert√©",
            "Avenue Jean Jaur√®s", "Rue Nationale", "Avenue Fran√ßois Mitterrand"
        ]
        id_offset = 10_000_000
    
    # √âlargir la liste des rues
    rues_expanded = []
    for base_rue in rues_base:
        for i in range(50):  # 50 variantes par rue de base
            rues_expanded.append(f"{base_rue} {['Nord', 'Sud', 'Est', 'Ouest', 'Centre'][i % 5]} {i//5 + 1}")
    
    output_file = f"sources/population_{city.lower()}.csv"
    
    # G√©n√©rer et sauvegarder par chunks
    chunks_generated = 0
    total_rows = 0
    
    for chunk_start in range(0, size, chunk_size):
        chunk_end = min(chunk_start + chunk_size, size)
        current_chunk_size = chunk_end - chunk_start
        
        chunk_data = []
        
        for i in range(current_chunk_size):
            global_id = id_offset + chunk_start + i + 1
            
            # Optimisation: utilisation de cycles pr√©-g√©n√©r√©s
            nom = random.choice(noms_cycle) if random.random() > 0.02 else None
            prenom = random.choice(prenoms_cycle) if random.random() > 0.015 else None
            
            # Adresse optimis√©e
            if random.random() > 0.01:
                numero = random.randint(1, 999)
                rue = random.choice(rues_expanded)
                adresse = f"{numero} {rue}"
            else:
                adresse = None
            
            # CSP avec distribution r√©aliste fran√ßaise
            csp_rand = random.random()
            if csp_rand < 0.03:  # 3% Agriculteurs
                csp = "1"
            elif csp_rand < 0.09:  # 6% Artisans/Commer√ßants
                csp = "2"
            elif csp_rand < 0.25:  # 16% Cadres
                csp = "3"
            elif csp_rand < 0.50:  # 25% Professions interm√©diaires
                csp = "4"
            elif csp_rand < 0.78:  # 28% Employ√©s
                csp = "5"
            elif csp_rand < 0.92:  # 14% Ouvriers
                csp = "6"
            elif csp_rand < 0.97:  # 5% Retrait√©s
                csp = "7"
            else:  # 3% Sans activit√©
                csp = "8"
            
            # Probl√®mes de qualit√© intentionnels
            if random.random() < 0.15:  # 15% de codes probl√©matiques
                problematic_csp = [
                    "cadre", "employ√©", "ouvrier", "retrait√©", "9", "0", "NC", "X", None
                ]
                csp = random.choice(problematic_csp)
            
            chunk_data.append({
                'ID_Personne': global_id,
                'Nom': nom,
                'Prenom': prenom,
                'Adresse': adresse,
                'CSP': csp
            })
        
        # Ajouter des doublons dans le chunk (3%)
        duplicates_count = int(current_chunk_size * 0.03)
        for _ in range(duplicates_count):
            if chunk_data:
                original = random.choice(chunk_data)
                duplicate = original.copy()
                # Variation mineure
                if random.random() < 0.5 and duplicate['Prenom']:
                    duplicate['Prenom'] = duplicate['Prenom'].upper()
                chunk_data.append(duplicate)
        
        # Convertir en DataFrame et sauvegarder
        chunk_df = pd.DataFrame(chunk_data)
        
        # Premi√®re fois: cr√©er le fichier avec header
        if chunks_generated == 0:
            chunk_df.to_csv(output_file, index=False, mode='w')
        else:
            # Ajouter sans header
            chunk_df.to_csv(output_file, index=False, mode='a', header=False)
        
        total_rows += len(chunk_df)
        chunks_generated += 1
        
        progress = (chunk_end / size) * 100
        print(f"  ‚úì Chunk {chunks_generated}: {chunk_end:,}/{size:,} ({progress:.1f}%) - {len(chunk_df):,} lignes")
        
        # Lib√©rer la m√©moire
        del chunk_data, chunk_df
    
    print(f"  ‚úÖ Population {city} compl√®te: {total_rows:,} lignes g√©n√©r√©es")
    return total_rows

def generate_massive_consumption(city, size, chunk_size=50000):
    """G√©n√©rer des donn√©es de consommation massives."""
    
    print(f"‚ö° G√©n√©ration MASSIVE de {size:,} relev√©s de consommation pour {city}...")
    
    if city.lower() == "paris":
        codes_postaux = [75001, 75002, 75003, 75004, 75005, 75006, 75007, 75008, 75009, 75010,
                        75011, 75012, 75013, 75014, 75015, 75016, 75017, 75018, 75019, 75020]
        id_offset = 0
    else:
        codes_postaux = [91000, 91100, 91200, 91300, 91400, 91500]
        id_offset = 20_000_000
    
    # Rues pr√©-g√©n√©r√©es pour optimisation
    rues_expanded = [f"Rue de la {word} {i}" for word in ["Paix", "R√©publique", "Libert√©", "Fraternit√©", "Justice"] for i in range(1, 201)]
    
    output_file = f"sources/consommation_{city.lower()}.csv"
    
    chunks_generated = 0
    total_rows = 0
    
    for chunk_start in range(0, size, chunk_size):
        chunk_end = min(chunk_start + chunk_size, size)
        current_chunk_size = chunk_end - chunk_start
        
        chunk_data = []
        
        for i in range(current_chunk_size):
            global_id = id_offset + chunk_start + i + 1
            
            # Num√©ro avec valeurs manquantes
            n = random.randint(1, 500) if random.random() > 0.02 else None
            
            # Rue al√©atoire
            nom_rue = random.choice(rues_expanded)
            
            # Code postal
            code_postal = random.choice(codes_postaux)
            
            # Consommation avec distribution r√©aliste
            # Distribution log-normale pour avoir des valeurs plus r√©alistes
            base_consumption = np.random.lognormal(mean=2.8, sigma=0.5)  # ~16 kWh/jour en moyenne
            
            # Facteur saisonnier
            seasonal = 1 + 0.4 * np.sin(2 * np.pi * random.random())
            consumption = base_consumption * seasonal
            
            # Valeurs aberrantes (1%)
            if random.random() < 0.01:
                if random.random() < 0.5:
                    consumption = random.uniform(0.1, 3)  # Tr√®s faible
                else:
                    consumption = random.uniform(80, 200)  # Tr√®s √©lev√©e
            
            # Valeurs impossibles (0.3%)
            if random.random() < 0.003:
                consumption = -abs(consumption)
            
            # Valeurs manquantes (2.5%)
            if random.random() < 0.025:
                consumption = None
            else:
                consumption = round(consumption, 1)
            
            chunk_data.append({
                'ID_Adr': global_id,
                'N': n,
                'Nom_Rue': nom_rue,
                'Code_Postal': code_postal,
                'NB_KW_Jour': consumption
            })
        
        # Doublons (2%)
        duplicates_count = int(current_chunk_size * 0.02)
        for _ in range(duplicates_count):
            if chunk_data:
                chunk_data.append(random.choice(chunk_data).copy())
        
        # Sauvegarder le chunk
        chunk_df = pd.DataFrame(chunk_data)
        
        if chunks_generated == 0:
            chunk_df.to_csv(output_file, index=False, mode='w')
        else:
            chunk_df.to_csv(output_file, index=False, mode='a', header=False)
        
        total_rows += len(chunk_df)
        chunks_generated += 1
        
        progress = (chunk_end / size) * 100
        print(f"  ‚úì Chunk {chunks_generated}: {chunk_end:,}/{size:,} ({progress:.1f}%) - {len(chunk_df):,} lignes")
        
        del chunk_data, chunk_df
    
    print(f"  ‚úÖ Consommation {city} compl√®te: {total_rows:,} lignes g√©n√©r√©es")
    return total_rows

def generate_massive_datasets(scale='medium'):
    """G√©n√©rer des datasets massifs selon la configuration choisie."""
    
    if scale not in SCALE_CONFIGS:
        print(f"‚ùå Configuration '{scale}' inconnue. Configurations disponibles:")
        for key, config in SCALE_CONFIGS.items():
            print(f"  {key}: {config['description']}")
        return
    
    config = SCALE_CONFIGS[scale]
    
    print("üöÄ G√âN√âRATION DE DATASETS MASSIFS")
    print("=" * 50)
    print(f"Configuration: {scale.upper()}")
    print(f"Description: {config['description']}")
    print(f"Population Paris: {config['population_paris']:,}")
    print(f"Population √âvry: {config['population_evry']:,}")
    print(f"Consommation Paris: {config['consumption_paris']:,}")
    print(f"Consommation √âvry: {config['consumption_evry']:,}")
    
    # Estimation de l'espace disque n√©cessaire
    estimated_size_mb = (
        config['population_paris'] * 0.045 +  # ~45 bytes par ligne population
        config['population_evry'] * 0.045 +
        config['consumption_paris'] * 0.040 +  # ~40 bytes par ligne consommation
        config['consumption_evry'] * 0.040
    )
    
    print(f"Espace disque estim√©: ~{estimated_size_mb:.0f} MB")
    print()
    
    # V√©rification de l'espace disque disponible
    disk_usage = os.statvfs('.') if hasattr(os, 'statvfs') else None
    if disk_usage:
        available_gb = (disk_usage.f_bavail * disk_usage.f_frsize) / (1024**3)
        if estimated_size_mb / 1024 > available_gb * 0.8:
            print(f"‚ö†Ô∏è  Attention: Espace disque possiblement insuffisant ({available_gb:.1f} GB disponibles)")
    
    confirm = input("Continuer la g√©n√©ration? (o/N): ").lower().strip()
    if confirm not in ['o', 'oui', 'y', 'yes']:
        print("‚ùå G√©n√©ration annul√©e")
        return
    
    start_time = datetime.now()
    
    # Cr√©er le dossier sources
    Path("sources").mkdir(exist_ok=True)
    
    # G√©n√©ration avec chunks pour √©conomiser la m√©moire
    chunk_size = min(50000, config['population_paris'] // 20)  # Chunks adaptatifs
    
    print(f"üìä Taille des chunks: {chunk_size:,} lignes")
    print()
    
    total_rows = 0
    
    # 1. Population Paris
    rows = generate_massive_population("Paris", config['population_paris'], chunk_size)
    total_rows += rows
    
    # 2. Population √âvry
    rows = generate_massive_population("√âvry", config['population_evry'], chunk_size)
    total_rows += rows
    
    # 3. Consommation Paris
    rows = generate_massive_consumption("Paris", config['consumption_paris'], chunk_size)
    total_rows += rows
    
    # 4. Consommation √âvry
    rows = generate_massive_consumption("√âvry", config['consumption_evry'], chunk_size)
    total_rows += rows
    
    # 5. Tables de r√©f√©rence (pas besoin de chunks)
    print("üë• G√©n√©ration CSP et IRIS...")
    
    # CSP (m√™me table que pr√©c√©demment)
    csp_data = [
        {"ID_CSP": i+1, "Desc": desc, "Salaire_Moyen": sal, "Salaire_Min": sal_min, "Salaire_Max": sal_max}
        for i, (desc, sal, sal_min, sal_max) in enumerate([
            ("Agriculteurs exploitants", 25000, 15000, 45000),
            ("Artisans, commer√ßants et chefs d'entreprise", 45000, 20000, 120000),
            ("Cadres et professions intellectuelles sup√©rieures", 65000, 40000, 150000),
            ("Professions interm√©diaires", 35000, 25000, 50000),
            ("Employ√©s", 28000, 20000, 40000),
            ("Ouvriers", 25000, 18000, 35000),
            ("Retrait√©s", 18000, 8000, 35000),
            ("Autres personnes sans activit√© professionnelle", 12000, 0, 25000)
        ])
    ]
    pd.DataFrame(csp_data).to_csv("sources/csp.csv", index=False)
    
    # IRIS √©tendu
    iris_data = []
    # Paris: 20 arrondissements √ó 50 IRIS = 1000 IRIS
    for arr in range(1, 21):
        for iris_num in range(1, 51):
            iris_data.append({
                'ID_Rue': (arr-1)*50 + iris_num,
                'ID_Ville': 'Paris',
                'ID_IRIS': f'IRIS_750{arr:02d}_{iris_num:03d}'
            })
    
    # √âvry: 200 IRIS
    for iris_num in range(1, 201):
        iris_data.append({
            'ID_Rue': 10000 + iris_num,
            'ID_Ville': 'Evry',
            'ID_IRIS': f'IRIS_91000_{iris_num:03d}'
        })
    
    pd.DataFrame(iris_data).to_csv("sources/iris.csv", index=False)
    total_rows += len(csp_data) + len(iris_data)
    
    print(f"  ‚úÖ {len(csp_data)} CSP + {len(iris_data)} IRIS g√©n√©r√©s")
    
    # Statistiques finales
    end_time = datetime.now()
    duration = end_time - start_time
    
    print()
    print("üìä STATISTIQUES FINALES")
    print("-" * 40)
    
    # Tailles des fichiers
    for filename in ['population_paris.csv', 'population_evry.csv', 
                    'consommation_paris.csv', 'consommation_evry.csv',
                    'csp.csv', 'iris.csv']:
        filepath = Path(f"sources/{filename}")
        if filepath.exists():
            size_mb = filepath.stat().st_size / (1024 * 1024)
            rows = sum(1 for _ in open(filepath)) - 1  # -1 pour le header
            print(f"{filename:25} : {size_mb:8.1f} MB ({rows:,} lignes)")
    
    total_size_mb = sum(f.stat().st_size for f in Path("sources").glob("*.csv")) / (1024 * 1024)
    
    print("-" * 40)
    print(f"{'TOTAL':25} : {total_size_mb:8.1f} MB ({total_rows:,} lignes)")
    print(f"Dur√©e de g√©n√©ration: {duration}")
    print(f"Vitesse: {total_rows / duration.total_seconds():.0f} lignes/seconde")
    print()
    print("‚úÖ G√©n√©ration massive termin√©e!")
    print("üöÄ Pr√™t pour des tests ETL haute performance!")

def main():
    """Point d'entr√©e principal avec interface utilisateur."""
    
    print("üéØ G√âN√âRATEUR DE DATASETS MASSIFS")
    print("Choisissez la taille des donn√©es √† g√©n√©rer:")
    print()
    
    for key, config in SCALE_CONFIGS.items():
        print(f"  {key:8} : {config['description']}")
    
    print()
    
    if len(sys.argv) > 1:
        scale = sys.argv[1].lower()
    else:
        scale = input("Entrez la configuration souhait√©e (small/medium/large/xlarge/xxlarge): ").lower().strip()
    
    if not scale:
        scale = 'small'
        print(f"Configuration par d√©faut: {scale}")
    
    generate_massive_datasets(scale)

if __name__ == "__main__":
    main()